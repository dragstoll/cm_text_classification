{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge pypdf2\n",
    "# conda install nltk \n",
    "# pip install HanTa\n",
    "\n",
    "import re, nltk, os, glob, docx, json, gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import wehs_helpers as wh\n",
    "# from HanTa import HanoverTagger as ht\n",
    "from docx2python import docx2python\n",
    "import win32com.client\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import gensim.corpora as corpora\n",
    "import spacy\n",
    "\n",
    "# import stopwords\n",
    "# nltk.download('stopwords')  \n",
    "os.name  # nt means windows\n",
    "pd.options.display.max_colwidth = 1000\n",
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "pd.set_option('display.max_rows', 500)\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "stemmer = SnowballStemmer(\"german\")\n",
    "import pickle\n",
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all reports\n",
    "rbs=pd.read_pickle('D:/phd/daten/python/db/rechenschaftsberichte_alles.pkl')\n",
    "#print list of columns\n",
    "# print(rbs.columns)\n",
    "\n",
    "df=rbs[['id_fall', 'fall_source_key', 'id_dokument', 'id_k', 'text', 'name','vorname' ,\n",
    "         'name_mutter', 'vorname_mutter', 'name_vater', 'vorname_vater',  ]]\n",
    "# die doppelten id_dokumente sind verschiedene Dokumente von verschiedenen Personen\n",
    "# df2 = df[df.id_dokument.duplicated(keep=False)].sort_values(\"id_dokument\")\n",
    "# print(df2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Violence Stems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction Text Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(stemmer.stem('berichten'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ViolenceFinder3:\n",
    "    # RE_POS_STEMS = ['polizei']\n",
    "    RE_POS_STEMS = ['gewalt','schl\\wg','übergriff']\n",
    "    \n",
    "    FALSE_POS_STEMS = ['schlagmüller','ausschlag','einschlag','eingeschlag','niederschlag','niedergeschlag',\n",
    "                      'schicksalsschlag','angeschlag','vorschlag','vorgeschlag','vorschläge','voranschlag',\n",
    "                      'ratschlag','ratschläge','schlagzeug','schleg']\n",
    "    CUT_PHRASES = ['(Welche Lösungen bzw. welches Vorgehen schlägt das Kind bzw. der/die Jugendliche vor)']\n",
    "    SPLITCHAR = '.'\n",
    "    SPLITPHRASES = ['Ausgangslage, Auftrag und Ziele\\n','Situation, Entwicklung\\n','Auftrag und Ziele\\n',\n",
    "                   'Vorgeschichte\\n']\n",
    "    SPLITPATTERN = r'((?<!(z\\.B|.\\s\\w|.\\W\\w|..\\d|Art|bzw|geb|Abs|Stv|etc))\\.|;|:|\\n\\w\\)|--|\\n\\n|\\d*\\.\\d*\\.\\d\\d\\d\\d\\n|\\{})'.format('|'.join(SPLITPHRASES))     # three characte expressions\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.sentences = [s for s in re.split(self.SPLITPATTERN, text) if s is not None]\n",
    "        self.pos_words = [self._find_pos_words(s) for s in self.sentences]\n",
    "        self.labels = [len(words)>0 for words in self.pos_words]\n",
    "                \n",
    "    def _find_pos_words(self,text):\n",
    "        for phrase in self.CUT_PHRASES:\n",
    "            text = text.lower().replace(phrase.lower(),'')\n",
    "        pattern = [r'\\b\\w*{}\\w*\\b'.format(stem) for stem in self.RE_POS_STEMS]\n",
    "        \n",
    "        pattern = r'(' + r'|'.join(pattern) + r')'\n",
    "        \n",
    "        pos_words = []\n",
    "        for word in re.findall(pattern, text.lower()):\n",
    "            if not np.any([fpword in word for fpword in self.FALSE_POS_STEMS]):\n",
    "                pos_words.append(word)\n",
    "        return pos_words\n",
    "    \n",
    "    def label(self):\n",
    "        return np.any(self.labels)\n",
    "        \n",
    "    def pos_sentences(self):\n",
    "        out = []\n",
    "        words0 = []\n",
    "        for ind in np.where(self.labels)[0]:\n",
    "            out.append(self.sentences[ind].replace('\\n','').strip()+self.SPLITCHAR)\n",
    "            words0.append(self.pos_words[ind])\n",
    "        \n",
    "        out3 = []\n",
    "        out5 = []\n",
    "        \n",
    "        for ind in np.where(self.labels)[0]:\n",
    "            try:\n",
    "                tempM1 = (self.sentences[ind-2].replace('\\n','').strip()+self.SPLITCHAR+' ')\n",
    "            except IndexError:\n",
    "                tempM1 = self.SPLITCHAR\n",
    "            \n",
    "            temp = (self.sentences[ind].replace('\\n','').strip()+self.SPLITCHAR+' ')\n",
    "            \n",
    "            try:\n",
    "                tempP1 =(self.sentences[ind+2].replace('\\n','').strip()+self.SPLITCHAR)\n",
    "            except IndexError:\n",
    "                tempP1 = self.SPLITCHAR\n",
    "            \n",
    "            try:\n",
    "                tempM2 = (self.sentences[ind-4].replace('\\n','').strip()+self.SPLITCHAR+' ')\n",
    "            except IndexError:\n",
    "                tempM2 = self.SPLITCHAR\n",
    "            try:\n",
    "                tempP2 =(self.sentences[ind+4].replace('\\n','').strip()+self.SPLITCHAR)\n",
    "            except IndexError:\n",
    "                tempP2 = self.SPLITCHAR \n",
    "                \n",
    "            out3_temp = tempM1 + temp + tempP1\n",
    "            out3.append(out3_temp)\n",
    "        #   print(out3_temp)\n",
    "            out5_temp = tempM2 + tempM1 + temp + tempP1 + tempP2\n",
    "            out5.append(out5_temp)\n",
    "            \n",
    "        return words0, out, out3, out5\n",
    "        \n",
    "    \n",
    "    def marked_text(self, ansi='\\033[44;33m', nul = \"\\033[m\"):\n",
    "        text = ''\n",
    "        for ii, s in enumerate(self.sentences):\n",
    "            if self.labels[ii]:\n",
    "                text += ansi + s + nul + self.SPLITCHAR\n",
    "            else:\n",
    "                text += s + self.SPLITCHAR\n",
    "        return text\n",
    "    \n",
    "    def pprint(self):\n",
    "        print(self.marked_text())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Sätze: 15605\n",
      "Anzahl Dokumente: 7542\n",
      "Anzahl doppelter Sätze: 2203\n",
      "Anzahl Sätze nach Filter: 13402\n",
      "Anzahl doppelter Sätze nach Filter: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentences = []\n",
    "words = []\n",
    "sentences3 = []\n",
    "sentences5 = []\n",
    "id_dict = {key:[] for key in ['id_dokument','id_fall', 'fall_source_key', 'id_k', 'name', 'vorname',]}\n",
    "for ii, row in df.iterrows():\n",
    "    pos_words, pos_sentences, pos_sentences3, pos_sentences5 = ViolenceFinder3(row['text']).pos_sentences()\n",
    "    \n",
    "    sentences += pos_sentences\n",
    "#     pos_sentences3 = ViolenceFinder(row['text']).pos_sentences3()\n",
    "    sentences3 += pos_sentences3\n",
    "#     print(pos_sentences3, len(pos_sentences3), pos_sentences, len(pos_sentences))\n",
    "    sentences5 += pos_sentences5\n",
    "    words +=pos_words\n",
    "    for key, el in id_dict.items():\n",
    "        el += [row[key]]*len(pos_sentences)\n",
    "    \n",
    "sent_df = pd.DataFrame()\n",
    "for key, el in id_dict.items():\n",
    "    sent_df[key] = el\n",
    "sent_df['Nsatz'] = sent_df.groupby(['id_dokument'])[['id_dokument']].count()['id_dokument'].reindex(sent_df['id_dokument']).values\n",
    "sent_df['Satz'] = sentences\n",
    "sent_df['pos_words'] = words\n",
    "sent_df['Satz3'] = sentences3\n",
    "sent_df['Satz5'] = sentences5\n",
    "print('Anzahl Sätze:',sent_df.shape[0])\n",
    "print('Anzahl Dokumente:',len(sent_df['id_dokument'].unique()))\n",
    "sent_df.head()\n",
    "\n",
    "\n",
    "#count doubles in sent_df columns\n",
    "print('Anzahl doppelter Sätze:',sent_df['Satz'].duplicated().sum())\n",
    "#filter out doubles in sent_df columns\n",
    "sent_df2 = sent_df.drop_duplicates(subset=['Satz'])\n",
    "print('Anzahl Sätze nach Filter:',sent_df2.shape[0])\n",
    "#check if there are still doubles\n",
    "print('Anzahl doppelter Sätze nach Filter:',sent_df2['Satz'].duplicated().sum())\n",
    "sent3_df = sent_df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' aggresivitat ', ' aggression ', ' aggressiv ', ' angreifen ', ' angriff ', ' anschreien ', ' aufdringlich ', ' bedrohen ', ' begrabscht ', ' belastigen ', ' beleidigen ', ' beruhrt ', ' beschimpfen ', ' bestraffung ', ' blaue ', ' blutige ', ' castagna ', ' einschuchtern ', ' einsperren ', ' erniedrigen ', ' faust ', ' flecken ', ' gebrochen ', ' gefahrden ', ' gefahrdung ', ' gekusst ', ' genitalien ', ' kindeswohl ', ' kindeswohlgefahrdung ', ' kneifen ', ' knochen ', ' korperlich ', ' kratzer ', ' lacherlich ', ' lippen ', ' missbrauch ', ' missbrauchen ', ' misshandeln ', ' misshandlung ', ' nackt ', ' nase ', ' ohrfeigen ', ' packen ', ' penis ', ' po ', ' polizei ', ' polizeibericht ', ' polizeilich ', ' polizeirapport ', ' prellung ', ' quetschung ', ' rote ', ' scheide ', ' schnittwunde ', ' schubsen ', ' schutteln ', ' sexuell ', ' stossen ', ' strafe ', ' strafverfahren ', ' vagina ', ' verbrennung ', ' verletzung ', ' verstauchung ', ' vorfall ', ' waffe ', ' weh ', ' werfen ', ' wurgen ']\n"
     ]
    }
   ],
   "source": [
    "# create a list with stemmed words of the list RE_POS_STEMS\n",
    "RE_POS_STEMS = [\n",
    "  ' Angriff ',\n",
    "        ' angreifen ',\n",
    "    ' aggressiv ',\n",
    "    ' Aggression ',\n",
    "    ' Aggresivität ',    \n",
    "' anschreien ',\n",
    "' aufdringlich ',\n",
    "# ' Auge ',\n",
    "# ' ausschliessen ',\n",
    "' bedrohen ',\n",
    "' begrabscht ',\n",
    "' Belästigen ',\n",
    "' beleidigen ',\n",
    "' berührt ',\n",
    "' beschimpfen ',\n",
    "' Bestraffung ',\n",
    "' blaue ',\n",
    "' blutige ',\n",
    "# ' eingeführt ',\n",
    "' Castagna ',\n",
    "' einschüchtern ',\n",
    "' einsperren ',\n",
    "' erniedrigen ',\n",
    "' Faust ',\n",
    "' Flecken ',\n",
    "' gebrochen ',\n",
    "' gefährden ',\n",
    "' Gefährdung ',\n",
    "' geküsst ',\n",
    "' Genitalien ',\n",
    "# ' gezwungen ',\n",
    "' kneifen ',\n",
    "' Knochen ',\n",
    "' körperlich ',\n",
    "' Kratzer ',\n",
    "' Kindeswohl ',\n",
    "' Kindeswohlgefährdung ',\n",
    "' lächerlich ',\n",
    "' Lippen ',\n",
    "' Misshandlung ',\n",
    "' misshandeln ',\n",
    "' missbrauchen ',\n",
    "' Missbrauch ',\n",
    "' nackt ',\n",
    "' Nase ',\n",
    "' ohrfeigen ',\n",
    "' packen ',\n",
    "' Penis ',\n",
    "' Po ',\n",
    "' Polizei ',\n",
    "' polizeilich ',\n",
    "' Polizeirapport ',\n",
    "' Polizeibericht ',\n",
    "' Prellung ',\n",
    "' Quetschung ',\n",
    "' rote ',\n",
    "' Scheide ',\n",
    "' Schnittwunde ',\n",
    "' schubsen ',\n",
    "' schütteln ',\n",
    "' sexuell ',\n",
    "' stossen ',\n",
    "' Strafe ',\n",
    "' Strafverfahren ',\n",
    "# ' treten ',\n",
    "' Vagina ',\n",
    "' Verbrennung ',\n",
    "' Verletzung ',\n",
    "' Verstauchung ',\n",
    "' Vorfall ',\n",
    "' Waffe ',\n",
    "' Weh ',\n",
    "' werfen ',\n",
    "' würgen ',\n",
    "]\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer(\"german\")\n",
    "stemmed_words = [stemmer.stem(word) for word in RE_POS_STEMS]\n",
    "stemmed_words = list(set(stemmed_words))\n",
    "stemmed_words.sort()\n",
    "print(stemmed_words)\n",
    "#add for every list item string '\\w'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Angriff ', ' angreifen ', ' aggressiv ', ' Aggression ', ' Aggresivität ', ' anschreien ', ' aufdringlich ', ' bedrohen ', ' begrabscht ', ' Belästigen ', ' beleidigen ', ' berührt ', ' beschimpfen ', ' Bestraffung ', ' blaue ', ' blutige ', ' Castagna ', ' einschüchtern ', ' einsperren ', ' erniedrigen ', ' Faust ', ' Flecken ', ' gebrochen ', ' gefährden ', ' Gefährdung ', ' geküsst ', ' Genitalien ', ' kneifen ', ' Knochen ', ' körperlich ', ' Kratzer ', ' Kindeswohl ', ' Kindeswohlgefährdung ', ' lächerlich ', ' Lippen ', ' Misshandlung ', ' misshandeln ', ' missbrauchen ', ' Missbrauch ', ' nackt ', ' Nase ', ' ohrfeigen ', ' packen ', ' Penis ', ' Po ', ' Polizei ', ' polizeilich ', ' Polizeirapport ', ' Polizeibericht ', ' Prellung ', ' Quetschung ', ' rote ', ' Scheide ', ' Schnittwunde ', ' schubsen ', ' schütteln ', ' sexuell ', ' stossen ', ' Strafe ', ' Strafverfahren ', ' Vagina ', ' Verbrennung ', ' Verletzung ', ' Verstauchung ', ' Vorfall ', ' Waffe ', ' Weh ', ' werfen ', ' würgen ']\n"
     ]
    }
   ],
   "source": [
    "print(RE_POS_STEMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#liste wurde korrigiert, die meisten Begriffe haben \\w erhalten damit weitere Endungen auch gefunden weden, eizelne Substantive wie Faust müssen als ganze Wörter gefunden werden deshalb ' Faust '\n",
    "stem_list=[' aggresivitat\\w', ' aggression\\w', ' aggressiv\\w', ' angreifen\\w', ' angriff\\w', ' anschreien\\w', ' aufdringlich\\w', ' bedrohen\\w', ' begrabscht\\w', ' belastigen\\w', ' beleidigen\\w', ' beruhrt\\w', ' beschimpfen\\w', ' bestraffung\\w', ' blaue\\w', ' blutige\\w', ' castagna\\w', ' einschuchtern\\w', ' einsperren\\w', ' erniedrigen\\w', ' faust', ' flecken\\w', ' gebrochen\\w', ' gefahrden\\w', ' gefahrdung\\w', ' gekusst\\w', ' genitalien\\w', ' kindeswohl\\w', ' kindeswohlgefahrdung\\w', ' kneifen\\w', ' knochen\\w', ' korperlich\\w', ' kratzer\\w', ' lacherlich\\w', ' lippen\\w', ' missbrauch\\w', ' missbrauchen\\w', ' misshandeln\\w', ' misshandlung\\w', ' nackt\\w', ' nase ', ' ohrfeigen\\w', ' packen\\w', ' penis\\w', ' po ', ' polizei\\w', ' polizeibericht\\w', ' polizeilich\\w', ' polizeirapport\\w', ' prellung\\w', ' quetschung\\w', ' rote\\w', '  scheide', ' schnittwunde\\w', ' schubsen\\w', ' schutteln\\w', ' sexuell\\w', ' stossen\\w', ' strafe\\w', ' strafverfahren\\w', ' vagina\\w', ' verbrennung\\w', ' verletzung\\w', ' verstauchung\\w', ' vorfall\\w', ' waffe\\w', ' weh ', ' werfen\\w', ' wurgen\\w',\n",
    "'Angriff', 'angreifen', 'aggressiv', 'Aggression', 'Aggresivität', 'anschreien', 'aufdringlich', 'bedrohen', 'begrabscht', 'Belästigen', 'beleidigen', 'berührt', 'beschimpfen', 'Bestraffung',  'blutige', 'Castagna', 'einschüchtern', 'einsperren', 'erniedrigen', ' Faust', ' Flecken', ' gebrochen', 'gefährden', 'Gefährdung', 'Genitalien', 'kneifen', 'Knochen', 'körperlich', 'Kratzer', 'Kindeswohl', 'Kindeswohlgefährdung', 'lächerlich',  'Misshandlung', 'misshandeln', 'missbrauchen', 'Missbrauch', 'nackt', ' Nase ', 'ohrfeigen', 'packen', ' Penis',  'Polizei', 'polizeilich', 'Polizeirapport', 'Polizeibericht', 'Prellung', 'Quetschung', ' Scheide ', 'Schnittwunde', 'schubsen', 'schütteln', 'sexuell', 'stossen', 'Strafe', 'Strafverfahren', 'Vagina', 'Verbrennung', 'Verletzung', 'Verstauchung', 'Vorfall', 'Waffe', ' Weh ', 'werfen', 'würgen ']\n",
    "# ['aggresivitat\\w', 'aggression\\w', 'aggressiv\\w', 'angreifen\\w', 'angriff\\w', 'anschreien\\w', 'aufdringlich\\w', 'bedrohen\\w', 'begrabscht\\w', 'belastigen\\w', 'beleidigen\\w', 'beruhrt\\w', 'beschimpfen\\w', 'bestraffung\\w', 'blaue\\w', 'blutige\\w', 'castagna\\w', 'einschuchtern\\w', 'einsperren\\w', 'erniedrigen\\w', 'faust ', 'flecken\\w', 'gebrochen\\w', 'gefahrden\\w', 'gefahrdung\\w', 'gekusst\\w', 'genitalien\\w', 'kindeswohl\\w', 'kindeswohlgefahrdung\\w', 'kneifen\\w', 'knochen\\w', 'korperlich\\w', 'kratzer\\w', 'lacherlich\\w', 'lippen\\w', 'missbrauch\\w', 'missbrauchen\\w', 'misshandeln\\w', 'misshandlung\\w', 'nackt\\w', 'nase ', 'ohrfeigen\\w', 'packen\\w', 'penis\\w', 'po ', 'polizei\\w', 'polizeibericht\\w', 'polizeilich\\w', 'polizeirapport\\w', 'prellung\\w', 'quetschung\\w', 'rote\\w', 'scheide ', 'schnittwunde\\w', 'schubsen\\w', 'schutteln\\w', 'sexuell\\w', 'stossen\\w', 'strafe\\w', 'strafverfahren\\w', 'vagina\\w', 'verbrennung\\w', 'verletzung\\w', 'verstauchung\\w', 'vorfall\\w', 'waffe\\w', 'weh ', 'werfen\\w', 'wurgen\\w',]\n",
    "# [' aggresivitat\\w', ' aggression\\w', ' aggressiv\\w', ' angreifen\\w', ' angriff\\w', ' anschreien\\w', ' aufdringlich\\w', ' bedrohen\\w', ' begrabscht\\w', ' belastigen\\w', ' beleidigen\\w', ' beruhrt\\w', ' beschimpfen\\w', ' bestraffung\\w', ' blaue\\w', ' blutige\\w', ' castagna\\w', ' einschuchtern\\w', ' einsperren\\w', ' erniedrigen\\w', ' faust ', ' flecken\\w', ' gebrochen\\w', ' gefahrden\\w', ' gefahrdung\\w', ' gekusst\\w', ' genitalien\\w', ' kindeswohl\\w', ' kindeswohlgefahrdung\\w', ' kneifen\\w', ' knochen\\w', ' korperlich\\w', ' kratzer\\w', ' lacherlich\\w', ' lippen\\w', ' missbrauch\\w', ' missbrauchen\\w', ' misshandeln\\w', ' misshandlung\\w', ' nackt\\w', ' nase ', ' ohrfeigen\\w', ' packen\\w', ' penis\\w', ' po ', ' polizei\\w', ' polizeibericht\\w', ' polizeilich\\w', ' polizeirapport\\w', ' prellung\\w', ' quetschung\\w', ' rote\\w', ' scheide ', ' schnittwunde\\w', ' schubsen\\w', ' schutteln\\w', ' sexuell\\w', ' stossen\\w', ' strafe\\w', ' strafverfahren\\w', ' vagina\\w', ' verbrennung\\w', ' verletzung\\w', ' verstauchung\\w', ' vorfall\\w', ' waffe\\w', ' weh ', ' werfen\\w', ' wurgen\\w',]\n",
    "\n",
    "class ViolenceFinderNeue:\n",
    "    RE_POS_STEMS = stem_list\n",
    "\n",
    "    FALSE_POS_STEMS = ['schlagmüller','ausschlag','einschlag','eingeschlag','niederschlag','niedergeschlag',\n",
    "                      'schicksalsschlag','angeschlag','vorschlag','vorgeschlag','vorschläge','voranschlag',\n",
    "                      'ratschlag','ratschläge','schlagzeug','schleg'\n",
    "                                            'abgetreten', \n",
    "'abtreten',\n",
    "'abgetreten', \n",
    "'abtretenden', \n",
    "'abzutreten', \n",
    "'angetreten', \n",
    "'antreten', \n",
    "'anzupacken', \n",
    "'anzustossen', \n",
    "'angestossen',\n",
    "'anzutreten', \n",
    "'aufgetreten', \n",
    "'aufgetretene', \n",
    "'aufgetretenen', \n",
    "'aufgezwungen', \n",
    "'auftreten', \n",
    "'auftretende', \n",
    "'auftretenden', \n",
    "'auftretender', \n",
    "'auftretens', \n",
    "'aufwerfen', \n",
    "'aufzutreten', \n",
    "'ausgetreten', \n",
    "'austreten', \n",
    "'auszutreten', \n",
    "'beigetreten', \n",
    "'beitreten', \n",
    "'beizutreten', \n",
    "'betreten', \n",
    "'eingetreten', \n",
    "'eingetretene', \n",
    "'eingetretenen', \n",
    "'einpacken', \n",
    "'eintreten', \n",
    "'einzupacken', \n",
    "'einzutreten', \n",
    "'entgegentreten', \n",
    "'entgegenzutreten', \n",
    "'entwerfen', \n",
    "'gegenübertreten', \n",
    "'gezwungenermassen', \n",
    "'herangetreten', \n",
    "'heraustreten', \n",
    "'inkrafttreten', \n",
    "'nichteintreten', \n",
    "'nichteintretensentscheid', \n",
    "'protect', \n",
    "'protection', \n",
    "'protest', \n",
    "'rotegg', \n",
    "'spaccarotella', \n",
    "'stellvertreten', \n",
    "'stellvertretend', \n",
    "'stellvertretende', \n",
    "'stellvertretenden', \n",
    "'stellvertretender', \n",
    "'übergetreten', \n",
    "'überzutreten', \n",
    "'ungezwungen', \n",
    "'ungezwungenen', \n",
    "'ungezwungener', \n",
    "'vertreten', \n",
    "'vertreten', 'vertreten', \n",
    "'vertretende', \n",
    "'vertretenden', \n",
    "'wegwerfen', \n",
    "'anpackende', \n",
    "'beinprotese', \n",
    "'blauenstrasse', \n",
    "'blauenweg', \n",
    "'brote', \n",
    "'broteinheiten', \n",
    "'brotes', \n",
    "'bürotermine', \n",
    "'dazugestossen', \n",
    "'elektrotech', \n",
    "'elektrotechnik', \n",
    "'elektrotechniker', \n",
    "'elektrotechnischen', \n",
    "'konktreten', \n",
    "'kürzertreten', \n",
    "'nachaussentreten', \n",
    "'protezione', \n",
    "'rotenbach', \n",
    "'rotenberg', \n",
    "'rotenbühler', 'rotenbühler', \n",
    "'rotenburger', \n",
    "'rotenflueh', \n",
    "'rotenfluh', \n",
    "'saroten', \n",
    "'schroten', \n",
    "'tourotel', \n",
    "'übertreten', \n",
    "'unberührt', \n",
    "'verkörperlichung', \n",
    "'vetreten', \n",
    "'zurückgetreten', \n",
    "'zurücktreten', \n",
    "'zusammenpacken', \n",
    "]\n",
    "    CUT_PHRASES = ['(Welche Lösungen bzw. welches Vorgehen schlägt das Kind bzw. der/die Jugendliche vor)']\n",
    "    SPLITCHAR = '.'\n",
    "    SPLITPHRASES = ['Ausgangslage, Auftrag und Ziele\\n','Situation, Entwicklung\\n','Auftrag und Ziele\\n',\n",
    "                   'Vorgeschichte\\n']\n",
    "    SPLITPATTERN = r'((?<!(z\\.B|.\\s\\w|.\\W\\w|..\\d|Art|bzw|geb|Abs|Stv|etc))\\.|;|:|\\n\\w\\)|--|\\n\\n|\\d*\\.\\d*\\.\\d\\d\\d\\d\\n|\\{})'.format('|'.join(SPLITPHRASES))     # three characte expressions\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.sentences = [s for s in re.split(self.SPLITPATTERN, text) if s is not None]\n",
    "        self.pos_words = [self._find_pos_words(s) for s in self.sentences]\n",
    "        self.labels = [len(words)>0 for words in self.pos_words]\n",
    "                \n",
    "    def _find_pos_words(self,text):\n",
    "        for phrase in self.CUT_PHRASES:\n",
    "            text = text.lower().replace(phrase.lower(),'')\n",
    "        pattern = [r'\\b\\w*{}\\w*\\b'.format(stem) for stem in self.RE_POS_STEMS]\n",
    "        pattern = r'(' + r'|'.join(pattern) + r')'\n",
    "        # print(pattern)\n",
    "        pos_words = []\n",
    "        for word in re.findall(pattern, text.lower()):\n",
    "            if not np.any([fpword in word for fpword in self.FALSE_POS_STEMS]):\n",
    "                pos_words.append(word)\n",
    "                # print(\"word:\", word)\n",
    "                # print(\"satz:\", text)\n",
    "        return pos_words\n",
    "    \n",
    "    def label(self):\n",
    "        return np.any(self.labels)\n",
    "        \n",
    "    def pos_sentences(self):\n",
    "        out = []\n",
    "        words0 = []\n",
    "        for ind in np.where(self.labels)[0]:\n",
    "            out.append(self.sentences[ind].replace('\\n','').strip()+self.SPLITCHAR)\n",
    "            words0.append(self.pos_words[ind])\n",
    "        \n",
    "        out3 = []\n",
    "        out5 = []\n",
    "        \n",
    "        for ind in np.where(self.labels)[0]:\n",
    "            try:\n",
    "                tempM1 = (self.sentences[ind-2].replace('\\n','').strip()+self.SPLITCHAR+' ')\n",
    "            except IndexError:\n",
    "                tempM1 = self.SPLITCHAR\n",
    "            \n",
    "            temp = (self.sentences[ind].replace('\\n','').strip()+self.SPLITCHAR+' ')\n",
    "            \n",
    "            try:\n",
    "                tempP1 =(self.sentences[ind+2].replace('\\n','').strip()+self.SPLITCHAR)\n",
    "            except IndexError:\n",
    "                tempP1 = self.SPLITCHAR\n",
    "            \n",
    "            try:\n",
    "                tempM2 = (self.sentences[ind-4].replace('\\n','').strip()+self.SPLITCHAR+' ')\n",
    "            except IndexError:\n",
    "                tempM2 = self.SPLITCHAR\n",
    "            try:\n",
    "                tempP2 =(self.sentences[ind+4].replace('\\n','').strip()+self.SPLITCHAR)\n",
    "            except IndexError:\n",
    "                tempP2 = self.SPLITCHAR \n",
    "                \n",
    "            out3_temp = tempM1 + temp + tempP1\n",
    "            out3.append(out3_temp)\n",
    "        #   print(out3_temp)\n",
    "            out5_temp = tempM2 + tempM1 + temp + tempP1 + tempP2\n",
    "            out5.append(out5_temp)\n",
    "            \n",
    "        return words0, out, out3, out5\n",
    "        \n",
    "    \n",
    "    def marked_text(self, ansi='\\033[44;33m', nul = \"\\033[m\"):\n",
    "        text = ''\n",
    "        for ii, s in enumerate(self.sentences):\n",
    "            if self.labels[ii]:\n",
    "                text += ansi + s + nul + self.SPLITCHAR\n",
    "            else:\n",
    "                text += s + self.SPLITCHAR\n",
    "        return text\n",
    "    \n",
    "\n",
    "    def pprint(self):\n",
    "        print(self.marked_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print 50 random rows with columns 'Satz' and 'pos_words'\n",
    "# sent3_df[['Satz','pos_words']].sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Sätze: 21584\n",
      "Anzahl Dokumente: 11390\n",
      "Anzahl doppelter Sätze: 2762\n",
      "Anzahl Sätze nach Filter: 18822\n",
      "Anzahl doppelter Sätze nach Filter: 0\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "words = []\n",
    "sentences3 = []\n",
    "sentences5 = []\n",
    "id_dict = {key:[] for key in ['id_dokument','id_fall', 'fall_source_key', 'id_k', 'name', 'vorname', 'name_mutter', 'vorname_mutter', 'name_vater', 'vorname_vater',]}\n",
    "for ii, row in df.iterrows():\n",
    "    pos_words, pos_sentences, pos_sentences3, pos_sentences5 = ViolenceFinderNeue(row['text']).pos_sentences()\n",
    "    \n",
    "    sentences += pos_sentences\n",
    "#     pos_sentences3 = ViolenceFinder(row['text']).pos_sentences3()\n",
    "    sentences3 += pos_sentences3\n",
    "#     print(pos_sentences3, len(pos_sentences3), pos_sentences, len(pos_sentences))\n",
    "    sentences5 += pos_sentences5\n",
    "    words +=pos_words\n",
    "    for key, el in id_dict.items():\n",
    "        el += [row[key]]*len(pos_sentences)\n",
    "    \n",
    "sent_df = pd.DataFrame()\n",
    "for key, el in id_dict.items():\n",
    "    sent_df[key] = el\n",
    "sent_df['Nsatz'] = sent_df.groupby(['id_dokument'])[['id_dokument']].count()['id_dokument'].reindex(sent_df['id_dokument']).values\n",
    "sent_df['Satz'] = sentences\n",
    "sent_df['pos_words'] = words\n",
    "sent_df['Satz3'] = sentences3\n",
    "sent_df['Satz5'] = sentences5\n",
    "print('Anzahl Sätze:',sent_df.shape[0])\n",
    "print('Anzahl Dokumente:',len(sent_df['id_dokument'].unique()))\n",
    "sent_df.head()\n",
    "#count doubles in sent_df columns\n",
    "print('Anzahl doppelter Sätze:',sent_df['Satz'].duplicated().sum())\n",
    "#filter out doubles in sent_df columns\n",
    "sent_df2 = sent_df.drop_duplicates(subset=['Satz'])\n",
    "print('Anzahl Sätze nach Filter:',sent_df2.shape[0])\n",
    "#check if there are still doubles\n",
    "print('Anzahl doppelter Sätze nach Filter:',sent_df2['Satz'].duplicated().sum())\n",
    "sentNeue_df = sent_df2.copy()\n",
    "# print(sentNeue_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               pos_words\n",
      "[körperlich]                        1448\n",
      "[aggressiv]                         1395\n",
      "[körperliche]                       1347\n",
      "[körperlichen]                       950\n",
      "[des kindeswohls]                    824\n",
      "[gefährden]                          730\n",
      "[körperlicher]                       304\n",
      "[stossen]                            182\n",
      "[des strafverfahrens]                153\n",
      "[gestossen]                          149\n",
      "[werfen]                             135\n",
      "[einem polizeieinsatz]               128\n",
      "[des kindeswohles]                   113\n",
      "[sexuell]                            105\n",
      "[packen]                              94\n",
      "[des vorfalls]                        94\n",
      "[ein aggressives]                     87\n",
      "[und verletzungen]                    86\n",
      "[verstossen]                          86\n",
      "[die verletzungen]                    81\n",
      "[gefährdend]                          80\n",
      "[aggressives]                         70\n",
      "[eine kindeswohlgefährdung]           69\n",
      "[mit aggressivem]                     65\n",
      "[beschimpfen]                         63\n",
      "[und aggressionen]                    63\n",
      "[schweizerischen roten]               60\n",
      "[auf sexuelle]                        60\n",
      "[der sexuellen]                       59\n",
      "[der polizeilichen]                   58\n",
      "[zu sexuellen]                        58\n",
      "[mit aggressionen]                    56\n",
      "[eine polizeiliche]                   56\n",
      "[mit polizeieinsatz]                  53\n",
      "[und aggressives]                     53\n",
      "[seine aggressionen]                  52\n",
      "[gefährdende]                         50\n",
      "[körperliche, körperlich]             50\n",
      "[gefährdenden]                        49\n",
      "[zustossen]                           49\n",
      "[sexuelle]                            48\n",
      "[keine kindeswohlgefährdung]          47\n",
      "[wegen sexuellen]                     47\n",
      "[sein aggressives]                    46\n",
      "[anschreien]                          45\n",
      "[berührt]                             45\n",
      "[von sexuellen]                       45\n",
      "[beleidigen]                          44\n",
      "[einer polizeilichen]                 43\n",
      "[und aggressiven]                     42\n",
      "[dieses vorfalls]                     41\n",
      "[bedrohen]                            41\n",
      "[sexuell missbraucht]                 41\n",
      "[zu polizeieinsätzen]                 39\n",
      "[das aggressive]                      38\n",
      "[und beleidigend]                     37\n",
      "[des vorfalles]                       37\n",
      "[die sexuellen]                       37\n",
      "[anpacken]                            36\n",
      "[zu aggressiven]                      36\n",
      "[ein polizeirapport]                  36\n",
      "[eines vorfalls]                      35\n",
      "[akute kindeswohlgefährdung]          35\n",
      "[seinen aggressionen]                 35\n",
      "[einer kindeswohlgefährdung]          34\n",
      "[der kindeswohlgefährdung]            34\n",
      "[des sexuellen,  missbrauchs]         34\n",
      "[einen polizeieinsatz]                33\n",
      "[aggressiven]                         33\n",
      "[die polizeiliche]                    33\n",
      "[wegen sexueller]                     32\n",
      "[kindswohlgefährdenden]               32\n",
      "[ein polizeieinsatz]                  32\n",
      "[körperliches]                        31\n",
      "[wurde polizeilich]                   31\n",
      "[eines sexuellen]                     30\n",
      "[polizeiliche]                        30\n",
      "[und aggressivem]                     29\n",
      "[selbstgefährdendes]                  29\n",
      "[sexueller]                           28\n",
      "[aggressivität]                       27\n",
      "[gemäss polizeirapport]               27\n",
      "[einen polizeirapport]                26\n",
      "[laufenden strafverfahrens]           26\n",
      "[aggressive]                          26\n",
      "[der polizeirapport]                  26\n",
      "[dem polizeiposten]                   26\n",
      "[zu aggressivem]                      25\n",
      "[ohrfeigen]                           25\n",
      "[mit polizeilicher]                   25\n",
      "[die kindeswohlgefährdung]            25\n",
      "[durch aggressives]                   25\n",
      "[dem polizeirapport]                  25\n",
      "[des sexuellen]                       25\n",
      "[mit aggressiven]                     25\n",
      "[der faust]                           24\n",
      "[und aggressivität]                   24\n",
      "[die aggressiven]                     23\n",
      "[fremdgefährdend]                     23\n",
      "[den sexuellen]                       23\n",
      "3579\n"
     ]
    }
   ],
   "source": [
    "#count values of column 'pos_words' and show all of them in output\n",
    "freq_liest=sentNeue_df['pos_words'].value_counts()\n",
    "freq_liest=pd.DataFrame(freq_liest)\n",
    "print(freq_liest.head(100))\n",
    "print(len(freq_liest))\n",
    "#save freq_liest to excel\n",
    "# freq_liest.to_excel('freq_liest.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Sätze in sentNeueOhne3_df: 16930\n",
      "Anzahl Sätze in sentNeueClean1_df: 16922\n",
      "Anzahl Sätze in sentNeueClean2_df: 16073\n",
      "Anzahl Sätze in sentNeueClean3_df: 15155\n",
      "Anzahl Sätze in sentNeueClean4_df: 14976\n",
      "Anzahl Sätze in sentNeueClean5_df: 14956\n",
      "Anzahl Sätze in sentNeueClean6_df: 14838\n",
      "Anzahl Sätze in sentNeueClean7_df: 14821\n"
     ]
    }
   ],
   "source": [
    "#join sent3_df and sentNeue_df based on Satz column and keep only rows that are not in sent3_df and save it as sentNeueOhne3_df\n",
    "sentNeueOhne3_df = sentNeue_df[~sentNeue_df['Satz'].isin(sent3_df['Satz'])]\n",
    "print('Anzahl Sätze in sentNeueOhne3_df:',sentNeueOhne3_df.shape[0])\n",
    "#drop rows that contain 'in Kontakt' in Satz column and and contain 'treten' in Satz column and save it in new df sentNeueClean1_df\n",
    "sentNeueClean1_df = sentNeueOhne3_df[~((sentNeueOhne3_df['Satz'].str.contains('treten')) & (sentNeueOhne3_df['Satz'].str.contains('in Kontakt')))]\n",
    "sentNeueClean2_df = sentNeueClean1_df[~((sentNeueClean1_df['Satz'].str.contains('[Kk]örp' )) & (sentNeueClean1_df['Satz'].str.contains('[Gg]esun' )))]\n",
    "sentNeueClean3_df = sentNeueClean2_df[~((sentNeueClean2_df['Satz'].str.contains('[Kk]örp' )) & (sentNeueClean2_df['Satz'].str.contains('[Ee]ntwick' )))]\n",
    "sentNeueClean4_df = sentNeueClean3_df[~((sentNeueClean3_df['Satz'].str.contains('[Gg]renz' )) & (sentNeueClean3_df['Satz'].str.contains('[Ss]toss' )))]\n",
    "sentNeueClean5_df = sentNeueClean4_df[~((sentNeueClean4_df['Satz'].str.contains('vor den Kopf')) & (sentNeueClean4_df['Satz'].str.contains('stoss' )))]\n",
    "\n",
    "sentNeueClean6_df = sentNeueClean5_df[~((sentNeueClean5_df['Satz'].str.contains('[Rr]ot' )) & (sentNeueClean5_df['Satz'].str.contains('[Kk]reuz')))]\n",
    "\n",
    "sentNeueClean7_df = sentNeueClean6_df[~((sentNeueClean6_df['Satz'].str.contains('[Rr]ot' )) & (sentNeueClean6_df['Satz'].str.contains('Faden')))]\n",
    "print('Anzahl Sätze in sentNeueClean1_df:',sentNeueClean1_df.shape[0])\n",
    "print('Anzahl Sätze in sentNeueClean2_df:',sentNeueClean2_df.shape[0])\n",
    "print('Anzahl Sätze in sentNeueClean3_df:',sentNeueClean3_df.shape[0])\n",
    "print('Anzahl Sätze in sentNeueClean4_df:',sentNeueClean4_df.shape[0])\n",
    "print('Anzahl Sätze in sentNeueClean5_df:',sentNeueClean5_df.shape[0])\n",
    "print('Anzahl Sätze in sentNeueClean6_df:',sentNeueClean6_df.shape[0])\n",
    "print('Anzahl Sätze in sentNeueClean7_df:',sentNeueClean7_df.shape[0])\n",
    "#save sentNeueClean7_df as csv and as pickle and as excel\n",
    "# sentNeueClean7_df.to_csv('sentNeueClean7aggr2_df.csv', index=False)\n",
    "# sentNeueClean7_df.to_pickle('sentNeueClean7aggr2_df.pkl')\n",
    "# sentNeueClean7_df.to_excel('sentNeueClean7aggr2_df.xlsx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge pypdf2\n",
    "# conda install nltk \n",
    "# pip install HanTa\n",
    "\n",
    "import re, nltk, os, glob, docx, json, gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import wehs_helpers as wh\n",
    "# from HanTa import HanoverTagger as ht\n",
    "from docx2python import docx2python\n",
    "import win32com.client\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import gensim.corpora as corpora\n",
    "import spacy\n",
    "\n",
    "# import stopwords\n",
    "# nltk.download('stopwords')  \n",
    "os.name  # nt means windows\n",
    "pd.options.display.max_colwidth = 1000\n",
    "nlp = spacy.load(\"de_core_news_lg\")\n",
    "pd.set_option('display.max_rows', 500)\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "stemmer = SnowballStemmer(\"german\")\n",
    "import pickle\n",
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id_dokument', 'id_fall', 'fall_source_key', 'id_k', 'name', 'vorname',\n",
       "       'nsatz', 'satz', 'pos_words', 'satz3', 'satz5'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path    \n",
    "saetze_3Begriffe = sent3_df\n",
    "# pd.read_pickle('GewaltSaetzeProzessiert_20230212.pkl')\n",
    "#change alle column names to lowercase\n",
    "saetze_3Begriffe.columns = map(str.lower, saetze_3Begriffe.columns)\n",
    "saetze_3Begriffe.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#keep only the columns 'id_dokument', 'id_k', 'nsatz', 'satz', 'satz3', 'satz5', 'name', 'vorname'\n",
    "# saetze_3Begriffe = saetze_3Begriffe[['id_dokument', 'id_k', 'nsatz', 'satz', 'satz3', 'satz5', 'name', 'vorname']]\n",
    "#sort columns in dataframe by alphabetical order\n",
    "# saetze_3Begriffe = saetze_3Begriffe.reindex(sorted(saetze_3Begriffe.columns), axis=1)\n",
    "# saetze_3Begriffe.columns\n",
    "#create new column datensatz with value 'drei Begriffe'\n",
    "saetze_3Begriffe['datensatz'] = 'drei Begriffe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id_dokument', 'id_fall', 'fall_source_key', 'id_k', 'name', 'vorname',\n",
       "       'nsatz', 'satz', 'pos_words', 'satz3', 'satz5'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read excel file sentNeueClean7joined2_df.xlsx and create dataframe\n",
    "saetze_weiterBegriffe = sentNeueClean7_df\n",
    "# pd.read_excel('sentNeueClean7joined2_df.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "\n",
    "#change alle column names to lowercase\n",
    "saetze_weiterBegriffe.columns = map(str.lower, saetze_weiterBegriffe.columns)\n",
    "#drop columns    'name_mutter', 'vorname_mutter', 'name_vater', 'vorname_vater',\n",
    "saetze_weiterBegriffe = saetze_weiterBegriffe.drop(['name_mutter', 'vorname_mutter', 'name_vater', 'vorname_vater'], axis=1)\n",
    "saetze_weiterBegriffe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#keep only the columns 'fall_source_key', 'id_dokument', 'id_fall', 'id_k', 'name','nsatz', 'pos_words', 'satz', 'satz3', 'satz5', 'vorname'\n",
    "# saetze_weiterBegriffe = saetze_weiterBegriffe[['fall_source_key', 'id_dokument', 'id_fall', 'id_k', 'name','nsatz', 'pos_words', 'satz', 'satz3', 'satz5', 'vorname']]\n",
    "#sort columns in dataframe by alphabetical order\n",
    "# saetze_weiterBegriffe = saetze_weiterBegriffe.reindex(sorted(saetze_weiterBegriffe.columns), axis=1)\n",
    "# saetze_weiterBegriffe.columns\n",
    "#create new column datensatz with value 'weitere Begriffe'\n",
    "saetze_weiterBegriffe['datensatz'] = 'weitere Begriffe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28223 entries, 0 to 28222\n",
      "Data columns (total 13 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   id_dokument             28223 non-null  int64 \n",
      " 1   id_fall                 28223 non-null  int64 \n",
      " 2   fall_source_key         28223 non-null  object\n",
      " 3   id_k                    28223 non-null  int64 \n",
      " 4   name                    28223 non-null  object\n",
      " 5   vorname                 28223 non-null  object\n",
      " 6   nsatz                   28223 non-null  int64 \n",
      " 7   satz                    28223 non-null  object\n",
      " 8   pos_words               28223 non-null  object\n",
      " 9   satz3                   28223 non-null  object\n",
      " 10  satz5                   28223 non-null  object\n",
      " 11  datensatz               28223 non-null  object\n",
      " 12  satz5_ohne_leerzeichen  28223 non-null  object\n",
      "dtypes: int64(4), object(9)\n",
      "memory usage: 2.8+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-37-e922c20f2419>:19: FutureWarning: null_counts is deprecated. Use show_counts instead\n",
      "  saetze_alleBegriffe.info(verbose=True, null_counts=True)\n"
     ]
    }
   ],
   "source": [
    "#join to dataframes saetze_3Begriffe and saetze_weiterBegriffe\n",
    "saetze_alleBegriffe = pd.concat([saetze_3Begriffe, saetze_weiterBegriffe], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "# replace the '|' with space in column 'satz5'\n",
    "# saetze_alleBegriffe['satz5'] = saetze_alleBegriffe['satz5'].str.replace('|', ' ')\n",
    "#replace four spaces with one space in column 'satz5'\n",
    "saetze_alleBegriffe['satz5'] = saetze_alleBegriffe['satz5'].str.replace('    ', ' ')\n",
    "#replace three spaces with one space in column 'satz5'\n",
    "saetze_alleBegriffe['satz5'] = saetze_alleBegriffe['satz5'].str.replace('   ', ' ')\n",
    "#replace two spaces with one space in column 'satz5'\n",
    "saetze_alleBegriffe['satz5'] = saetze_alleBegriffe['satz5'].str.replace('  ', ' ')\n",
    "\n",
    "#remove all spaces and tabs in column 'satz5' and save it in new column 'satz5_ohne_leerzeichen'\n",
    "saetze_alleBegriffe['satz5_ohne_leerzeichen'] = saetze_alleBegriffe['satz5'].str.replace(' ', '')\n",
    "saetze_alleBegriffe['satz5_ohne_leerzeichen'] = saetze_alleBegriffe['satz5_ohne_leerzeichen'].str.replace('\\t', '')\n",
    "#show all columns and missing values\n",
    "saetze_alleBegriffe.info(verbose=True, null_counts=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4397 entries, 0 to 4396\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  4397 non-null   int64 \n",
      " 1   label       4397 non-null   int64 \n",
      " 2   sentence    4396 non-null   object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 103.2+ KB\n",
      "4    1328\n",
      "1    1036\n",
      "0     876\n",
      "2     872\n",
      "3     285\n",
      "Name: label, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4397 entries, 0 to 4396\n",
      "Data columns (total 4 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   annotiert               4397 non-null   int64 \n",
      " 1   satz5_annotiert         4396 non-null   object\n",
      " 2   annotiert_string        4397 non-null   object\n",
      " 3   satz5_ohne_leerzeichen  4396 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 137.5+ KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-40-d94646a2f40a>:8: FutureWarning: null_counts is deprecated. Use show_counts instead\n",
      "  train_test_data.info(verbose=True, null_counts=True)\n",
      "<ipython-input-40-d94646a2f40a>:40: FutureWarning: null_counts is deprecated. Use show_counts instead\n",
      "  print(train_test_data.info(verbose=True, null_counts=True))\n"
     ]
    }
   ],
   "source": [
    "#load excel table train_data.xlsx into dataframe train_data\n",
    "train_data = pd.read_excel('train_data.xlsx', sheet_name='Sheet1')\n",
    "#load excel table test_data.xlsx into dataframe test_data\n",
    "test_data = pd.read_excel('test_data.xlsx', sheet_name='Sheet1')\n",
    "#join dataframes train_data and test_data\n",
    "train_test_data = pd.concat([train_data, test_data], axis=0, ignore_index=True)\n",
    "#show all columns and missing values\n",
    "train_test_data.info(verbose=True, null_counts=True)\n",
    "#count label values\n",
    "print(train_test_data['label'].value_counts())\n",
    "#recode label values 0 to 'physiche Gewalt', 1 to 'häusliche Gewalt', 2 to 'ausgeübte Gewalt', 3 to 'sexuelle Gewalt' and 4 to 'keine Gewalt' in a new column 'annotiert_string'\n",
    "train_test_data['annotiert_string'] = train_test_data['label'].replace({0: 'physiche Gewalt', 1: 'häusliche Gewalt', 2: 'ausgeübte Gewalt', 3: 'sexuelle Gewalt', 4: 'keine Gewalt'})\n",
    "#count annotiert_string values\n",
    "train_test_data['annotiert_string'].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#rename column label to annotiert and senctence to satz5\n",
    "train_test_data.rename(columns={'label': 'annotiert', 'sentence': 'satz5'}, inplace=True)\n",
    "\n",
    "#replace four spaces with one space in column 'satz5'\n",
    "train_test_data['satz5'] = train_test_data['satz5'].str.replace('    ', ' ')\n",
    "#replace three spaces with one space in column 'satz5'\n",
    "train_test_data['satz5'] = train_test_data['satz5'].str.replace('   ', ' ')\n",
    "#replace two spaces with one space in column 'satz5'\n",
    "train_test_data['satz5'] = train_test_data['satz5'].str.replace('  ', ' ')\n",
    "train_test_data['satz5'] = train_test_data['satz5'].str.replace('  ', ' ')\n",
    "train_test_data['satz5'] = train_test_data['satz5'].str.replace('  ', ' ')\n",
    "train_test_data['satz5'] = train_test_data['satz5'].str.replace('  ', ' ')\n",
    "#remove all spaces and tabs in column 'satz5' and save it in new column 'satz5_ohne_leerzeichen'\n",
    "train_test_data['satz5_ohne_leerzeichen'] = train_test_data['satz5'].str.replace(' ', '')\n",
    "\n",
    "train_test_data['satz5_ohne_leerzeichen'] = train_test_data['satz5_ohne_leerzeichen'].str.replace('\\t', '')\n",
    "\n",
    "#rename column 'satz5' to 'satz5_annotiert'\n",
    "train_test_data.rename(columns={'satz5': 'satz5_annotiert'}, inplace=True)\n",
    "#drop column Unnamed: 0\n",
    "train_test_data.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "print(train_test_data.info(verbose=True, null_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 28223 entries, 0 to 28222\n",
      "Data columns (total 16 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   id_dokument             28223 non-null  int64  \n",
      " 1   id_fall                 28223 non-null  int64  \n",
      " 2   fall_source_key         28223 non-null  object \n",
      " 3   id_k                    28223 non-null  int64  \n",
      " 4   name                    28223 non-null  object \n",
      " 5   vorname                 28223 non-null  object \n",
      " 6   nsatz                   28223 non-null  int64  \n",
      " 7   satz                    28223 non-null  object \n",
      " 8   pos_words               28223 non-null  object \n",
      " 9   satz3                   28223 non-null  object \n",
      " 10  satz5                   28223 non-null  object \n",
      " 11  datensatz               28223 non-null  object \n",
      " 12  satz5_ohne_leerzeichen  28223 non-null  object \n",
      " 13  annotiert               4396 non-null   float64\n",
      " 14  satz5_annotiert         4396 non-null   object \n",
      " 15  annotiert_string        4396 non-null   object \n",
      "dtypes: float64(1), int64(4), object(11)\n",
      "memory usage: 3.7+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-41-02009c8226e9>:4: FutureWarning: null_counts is deprecated. Use show_counts instead\n",
      "  saetze_alleBegriffe_annotiert.info(verbose=True, null_counts=True)\n"
     ]
    }
   ],
   "source": [
    "#join dataframes saetze_alleBegriffe and train_test_data on column 'satz5' and save it in dataframe saetze_alleBegriffe_annotiert\n",
    "saetze_alleBegriffe_annotiert = pd.merge(saetze_alleBegriffe, train_test_data, on='satz5_ohne_leerzeichen', how='left')\n",
    "#show all columns and missing values\n",
    "saetze_alleBegriffe_annotiert.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #show me an example of fuzzy matching to dataframes \n",
    "# # !pip install fuzzy_pandas\n",
    "\n",
    "# #drop rows with missing values in column satz5 in both dataframes\n",
    "# saetze_alleBegriffe.dropna(subset=['satz5'], inplace=True)\n",
    "# train_test_data.dropna(subset=['satz5'], inplace=True)\n",
    "# #show all columns and missing values\n",
    "# import fuzzy_pandas as fpd\n",
    "\n",
    "\n",
    "# saetze_alleBegriffe_annotiert = fpd.fuzzy_merge(saetze_alleBegriffe, train_test_data,\n",
    "#             left_on='satz5',\n",
    "#             right_on='satz5',\n",
    "#             method='levenshtein',\n",
    "#             threshold=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "saetze_alleBegriffe_annotiert.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 28223 entries, 0 to 28222\n",
      "Data columns (total 16 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   id_dokument             28223 non-null  int64  \n",
      " 1   id_fall                 28223 non-null  int64  \n",
      " 2   fall_source_key         28223 non-null  object \n",
      " 3   id_k                    28223 non-null  int64  \n",
      " 4   name                    28223 non-null  object \n",
      " 5   vorname                 28223 non-null  object \n",
      " 6   nsatz                   28223 non-null  int64  \n",
      " 7   satz                    28223 non-null  object \n",
      " 8   pos_words               28223 non-null  object \n",
      " 9   satz3                   28223 non-null  object \n",
      " 10  satz5                   28223 non-null  object \n",
      " 11  datensatz               28223 non-null  object \n",
      " 12  satz5_ohne_leerzeichen  28223 non-null  object \n",
      " 13  annotiert               4396 non-null   float64\n",
      " 14  satz5_annotiert         4396 non-null   object \n",
      " 15  annotiert_string        4396 non-null   object \n",
      "dtypes: float64(1), int64(4), object(11)\n",
      "memory usage: 3.7+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-42-72d40aae49b5>:2: FutureWarning: null_counts is deprecated. Use show_counts instead\n",
      "  saetze_alleBegriffe_annotiert.info(verbose=True, null_counts=True)\n"
     ]
    }
   ],
   "source": [
    "#print all columns and missing values\n",
    "saetze_alleBegriffe_annotiert.info(verbose=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>annotiert_string</th>\n",
       "      <th>ausgeübte Gewalt</th>\n",
       "      <th>häusliche Gewalt</th>\n",
       "      <th>keine Gewalt</th>\n",
       "      <th>physiche Gewalt</th>\n",
       "      <th>sexuelle Gewalt</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datensatz</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>drei Begriffe</th>\n",
       "      <td>680</td>\n",
       "      <td>939</td>\n",
       "      <td>816</td>\n",
       "      <td>787</td>\n",
       "      <td>187</td>\n",
       "      <td>3409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weitere Begriffe</th>\n",
       "      <td>192</td>\n",
       "      <td>96</td>\n",
       "      <td>512</td>\n",
       "      <td>89</td>\n",
       "      <td>98</td>\n",
       "      <td>987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>872</td>\n",
       "      <td>1035</td>\n",
       "      <td>1328</td>\n",
       "      <td>876</td>\n",
       "      <td>285</td>\n",
       "      <td>4396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "annotiert_string  ausgeübte Gewalt  häusliche Gewalt  keine Gewalt  \\\n",
       "datensatz                                                            \n",
       "drei Begriffe                  680               939           816   \n",
       "weitere Begriffe               192                96           512   \n",
       "All                            872              1035          1328   \n",
       "\n",
       "annotiert_string  physiche Gewalt  sexuelle Gewalt   All  \n",
       "datensatz                                                 \n",
       "drei Begriffe                 787              187  3409  \n",
       "weitere Begriffe               89               98   987  \n",
       "All                           876              285  4396  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create crosstab with datensatz and annotiert_string\n",
    "pd.crosstab(saetze_alleBegriffe_annotiert['datensatz'], saetze_alleBegriffe_annotiert['annotiert_string'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save saetze_alleBegriffe_annotiert as csv and as pickle and as excel with name  saetze_alleBegriffe_annotiert_20230829\n",
    "saetze_alleBegriffe_annotiert.to_csv('saetze_alleBegriffe_annotiert_20230829.csv', index=False)\n",
    "saetze_alleBegriffe_annotiert.to_pickle('saetze_alleBegriffe_annotiert_20230829.pkl')\n",
    "saetze_alleBegriffe_annotiert.to_excel('saetze_alleBegriffe_annotiert_20230829.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
