{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60199b85",
   "metadata": {},
   "source": [
    "# Extraction Text Sections with violence stems words\n",
    " \n",
    "## Literatur\n",
    "- https://textmining.wp.hs-hannover.de/Preprocessing.html\n",
    "- https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0\n",
    "- https://neptune.ai/blog/pyldavis-topic-modelling-exploration-tool-that-every-nlp-data-scientist-should-know\n",
    "\n",
    "## Ideen\n",
    "-  To determine if there is a reference to violence in the text, only certain parts of the text are relevant. Typically, the reference to violence can be identified by keywords such as violence, hitting, assault, aggressive, women's shelter(?), protective measure, police surveillance, fighting, psychological pressure, ...\n",
    "- Nur Teile des Textes sind relevant um zu bestimmen, ob es einen Bezug zu Gewalt gibt\n",
    "- Typischerweise zeigt sich der Bezug zu Gewalt an Stichworten: Gewalt, schlagen, übergriff, aggressiv, Frauenhaus(?), Schutzmassnahme, Polizeiüberwachung, aufeinander losgehen, psychischer Druck, ... \n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09a72529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge pypdf2\n",
    "# conda install nltk \n",
    "# pip install HanTa\n",
    "\n",
    "import re, nltk, os, glob, docx, json, gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import wehs_helpers as wh\n",
    "# from HanTa import HanoverTagger as ht\n",
    "from docx2python import docx2python\n",
    "import win32com.client\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import gensim.corpora as corpora\n",
    "\n",
    "# import stopwords\n",
    "# nltk.download('stopwords')  \n",
    "os.name  # nt means windows\n",
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef36021",
   "metadata": {},
   "source": [
    "## Analysis Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe511246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load meta data about the reports\n",
    "mdf = []\n",
    "for region in ['norden', 'osten', 'sueden', 'westen']:\n",
    "    meta_path = '../../../Daten_pcm_export/meta_infos_rbs_{}.csv'.format(region)\n",
    "    mdf.append(pd.read_csv(meta_path,delimiter=';').assign(region=region))\n",
    "\n",
    "mdf = pd.concat(mdf, axis=0).drop_duplicates('ID_DOKUMENT').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93feb7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdf['RBS_REIHENFOLGE'][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b7144c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RBS_REIHENFOLGE</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0            count\n",
       "RBS_REIHENFOLGE       \n",
       "1                15057\n",
       "2                 8148\n",
       "3                 4302\n",
       "4                 2360\n",
       "5                 1300\n",
       "6                  697\n",
       "7                  368\n",
       "8                  179\n",
       "9                   63\n",
       "10                  34\n",
       "11                  13\n",
       "12                   6\n",
       "13                   4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the number of reports per clients case\n",
    "wh.one_count(mdf,'RBS_REIHENFOLGE')\n",
    "pd.crosstab(index=mdf['RBS_REIHENFOLGE'], columns='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324560ba",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c7d4748",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = [pd.read_csv(path,index_col=0) for path in glob.glob('data/*.csv')]\n",
    "df = pd.concat(df).fillna({'text':''})\n",
    "df['textlen'] = df.text.apply(len)\n",
    "\n",
    "df[\"text\"][:1]\n",
    "# df['nn_lemmas'][:1]\n",
    "# parse list, Hochkommas werden ersetzt, String mit allen Lemmas erstellt\n",
    "def parse_list(s):\n",
    "    for ch in \"'[]\":\n",
    "        s = s.replace(ch,'')\n",
    "    return s.split(', ')\n",
    "\n",
    "# df['nn_lemmas'] = df['nn_lemmas'].apply(parse_list)\n",
    "# df['vv_lemmas'] = df['vv_lemmas'].apply(parse_list)\n",
    "# df['adja_lemmas'] = df['adja_lemmas'].apply(parse_list)\n",
    "# df['nn_lemmas'][:1]\n",
    "# remove duplicates\n",
    "df = df.drop_duplicates('ID_DOKUMENT').reset_index(drop=True)\n",
    "# append new metadata\n",
    "id_col = 'ID_DOKUMENT'\n",
    "tmp = mdf.set_index(id_col).reindex(df[id_col])\n",
    "for col in tmp.columns:\n",
    "    if col not in df.columns:\n",
    "        df[col] = tmp[col].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dedeb3e",
   "metadata": {},
   "source": [
    "## Identifying sentences with violence\n",
    "\n",
    "False positives:\n",
    "- Hitting: suggestion, suggestions, suggested, I suggest, fights, affected, division proposal\n",
    "- Pressure: under pressure, impression, expression\n",
    "\n",
    "Others:\n",
    "- started\n",
    "- psychological pressure\n",
    "\n",
    "Difficult cases:\n",
    "- State of mind, wishes, will (Which solutions or approach does the child or adolescent suggest)\n",
    "## Sätze mit Gewalt identifizieren\n",
    "\n",
    "Falsch positive:\n",
    "- Schlagen: Vorschlag, Vorschläge, schlug vor, schlage ich vor, Schlägereien, angeschlagen, Teilungsvorschlag\n",
    "- Druck: Auf druck hin, Eindruck, Ausdruck, \n",
    "\n",
    "Weitere:\n",
    "- losgegangen\n",
    "- psyschischer Druck\n",
    "\n",
    "Schwierige Fälle:\n",
    "- Befindlichkeit, Wünsche, Wille (Welche Lösungen bzw. welches Vorgehen schlägt das Kind bzw. der/die Jugendliche vor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e97868e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[:5,[\"text\", \"VORNAME\", \"NAME\", \"VORNAME_MUTTER\", \"NAME_MUTTER\", \"VORNAME_VATER\", \"NAME_VATER\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e988f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = df.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "090b481e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# search for violence stems in the text and extraxt the sentence and the two sentences before and after the violence stem\n",
    "class ViolenceFinder:\n",
    "    RE_POS_STEMS = ['gewalt','schl\\wg','übergriff']\n",
    "    FALSE_POS_STEMS = ['schlagmüller','ausschlag','einschlag','eingeschlag','niederschlag','niedergeschlag',\n",
    "                      'schicksalsschlag','angeschlag','vorschlag','vorgeschlag','vorschläge','voranschlag',\n",
    "                      'ratschlag','ratschläge','schlagzeug','schleg']\n",
    "    CUT_PHRASES = ['(Welche Lösungen bzw. welches Vorgehen schlägt das Kind bzw. der/die Jugendliche vor)']\n",
    "    SPLITCHAR = '. | '\n",
    "    SPLITCHARFULLSTOP = '.'\n",
    "    SPLITPHRASES = ['Ausgangslage, Auftrag und Ziele\\n','Situation, Entwicklung\\n','Auftrag und Ziele\\n',\n",
    "                   'Vorgeschichte\\n']\n",
    "    SPLITPATTERN = r'((?<!(z\\.B|.\\s\\w|.\\W\\w|..\\d|Art|bzw|geb|Abs|Stv|etc))\\.|;|:|\\n\\w\\)|--|\\n\\n|\\d*\\.\\d*\\.\\d\\d\\d\\d\\n|\\{})'.format('|'.join(SPLITPHRASES))     # three characte expressions\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        \n",
    "        self.sentences = [s for s in re.split(self.SPLITPATTERN, text) if s is not None]\n",
    "        \n",
    "        \n",
    "        self.pos_words = [self._find_pos_words(s) for s in self.sentences]\n",
    "        self.labels = [len(words)>0 for words in self.pos_words]\n",
    "\n",
    "                \n",
    "    def _find_pos_words(self,text):\n",
    "        for phrase in self.CUT_PHRASES:\n",
    "            text = text.lower().replace(phrase.lower(),'')\n",
    "        pattern = [r'\\b\\w*{}\\w*\\b'.format(stem) for stem in self.RE_POS_STEMS]\n",
    "        pattern = r'(' + r'|'.join(pattern) + r')'\n",
    "        pos_words = []\n",
    "        for word in re.findall(pattern, text.lower()):\n",
    "            if not np.any([fpword in word for fpword in self.FALSE_POS_STEMS]):\n",
    "                pos_words.append(word)\n",
    "        return pos_words\n",
    "    \n",
    "    def label(self):\n",
    "        return np.any(self.labels)\n",
    "        \n",
    "    def pos_sentences(self):\n",
    "        out = []        \n",
    "        for ind in np.where(self.labels)[0]:\n",
    "            out.append(self.sentences[ind].replace('\\n','').strip()+self.SPLITCHARFULLSTOP)\n",
    "#             print(type(out))\n",
    "        out3 = []\n",
    "        out5 = []\n",
    "        for ind in np.where(self.labels)[0]:\n",
    "            try:\n",
    "                tempM1 = (self.sentences[ind-2].replace('\\n','').strip()+self.SPLITCHAR+' ')\n",
    "            except IndexError:\n",
    "                tempM1 = self.SPLITCHAR\n",
    "            \n",
    "            temp = (self.sentences[ind].replace('\\n','').strip()+self.SPLITCHAR+' ')\n",
    "            \n",
    "            try:\n",
    "                tempP1 =(self.sentences[ind+2].replace('\\n','').strip()+self.SPLITCHAR)\n",
    "            except IndexError:\n",
    "                tempP1 = self.SPLITCHAR\n",
    "            \n",
    "            try:\n",
    "                tempM2 = (self.sentences[ind-4].replace('\\n','').strip()+self.SPLITCHAR+' ')\n",
    "            except IndexError:\n",
    "                tempM2 = self.SPLITCHAR\n",
    "            try:\n",
    "                tempP2 =(self.sentences[ind+4].replace('\\n','').strip()+self.SPLITCHAR)\n",
    "            except IndexError:\n",
    "                tempP2 = self.SPLITCHAR \n",
    "                \n",
    "            out3_temp = tempM1 + temp + tempP1\n",
    "            out3.append(out3_temp)\n",
    "#             print(out3_temp)\n",
    "            out5_temp = tempM2 + tempM1 + temp + tempP1 + tempP2\n",
    "            out5.append(out5_temp)\n",
    "            \n",
    "        return out, out3, out5\n",
    "#     def pos_sentences3(self):       \n",
    "#         out3 = []\n",
    "#         for ind in np.where(self.labels)[0]:\n",
    "#             try:\n",
    "#                 tempM1 = (self.sentences[ind-2].replace('\\n','').strip()+self.SPLITCHAR+' ')\n",
    "#             except IndexError:\n",
    "#                 tempM1 = '.'\n",
    "            \n",
    "#             temp = (self.sentences[ind].replace('\\n','').strip()+self.SPLITCHAR+' ')\n",
    "            \n",
    "#             try:\n",
    "#                 tempP1 =(self.sentences[ind+2].replace('\\n','').strip()+self.SPLITCHAR)\n",
    "#             except IndexError:\n",
    "#                 tempP1 = '.'\n",
    "                \n",
    "            \n",
    "#             out3.append(tempM1 + temp + tempP1)\n",
    "# #             print(type(out3))\n",
    "#         return out3\n",
    "\n",
    "    def marked_text(self, ansi='\\033[44;33m', nul = \"\\033[0m\"):\n",
    "        text = ''\n",
    "        for ii, s in enumerate(self.sentences):\n",
    "            if self.labels[ii]:\n",
    "                text += ansi + s + nul + self.SPLITCHAR\n",
    "            else:\n",
    "                text += s + self.SPLITCHAR        \n",
    "        return text\n",
    "    \n",
    "    def pprint(self):\n",
    "        print(self.marked_text())\n",
    "        \n",
    "    def ppprint(self):\n",
    "        print(self.sentencesMinus1, self.sentences, self.sentencesPlus1)\n",
    "    \n",
    "vf = ViolenceFinder(df.text[2])\n",
    "# vf.pprint()\n",
    "# vf.ppprint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3191d0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the sentences with violence stems in a dataframe\n",
    "sentences = []\n",
    "sentences3 = []\n",
    "sentences5 = []\n",
    "id_dict = {key:[] for key in ['ID_DOKUMENT','ID_K','RBS_REIHENFOLGE']}\n",
    "for ii, row in df.iterrows():\n",
    "    pos_sentences, pos_sentences3, pos_sentences5 = ViolenceFinder(row['text']).pos_sentences()\n",
    "    sentences += pos_sentences\n",
    "#     pos_sentences3 = ViolenceFinder(row['text']).pos_sentences3()\n",
    "    sentences3 += pos_sentences3\n",
    "#     print(pos_sentences3, len(pos_sentences3), pos_sentences, len(pos_sentences))\n",
    "    sentences5 += pos_sentences5\n",
    "\n",
    "    for key, el in id_dict.items():\n",
    "        el += [row[key]]*len(pos_sentences)\n",
    "    \n",
    "\n",
    "\n",
    "sent_df = pd.DataFrame()\n",
    "for key, el in id_dict.items():\n",
    "    sent_df[key] = el\n",
    "sent_df['Nsatz'] = sent_df.groupby(['ID_K'])[['ID_K']].count()['ID_K'].reindex(sent_df['ID_K']).values\n",
    "sent_df['Gewalt'] = 1\n",
    "sent_df['Gegen Kind'] = np.nan\n",
    "sent_df['Satz'] = sentences\n",
    "sent_df['Satz3'] = sentences3\n",
    "sent_df['Satz5'] = sentences5\n",
    " \n",
    "print('Anzahl Sätze:',sent_df.shape[0])\n",
    "print('Anzahl Dokumente:',len(sent_df['ID_DOKUMENT'].unique()))\n",
    "sent_df.head()\n",
    "sent_df.ID_DOKUMENT.is_unique\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b6a4767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the metadata of the clients case to the sentences\n",
    "sent2_df= pd.merge(sent_df, df[['ID_DOKUMENT', 'ID_COMBO_ANREDE', 'VORNAME', 'NAME', 'VORNAME_MUTTER', 'NAME_MUTTER', 'VORNAME_VATER', 'NAME_VATER']], on=\"ID_DOKUMENT\", how=\"left\")\n",
    "pd.options.display.max_colwidth = 100000\n",
    "# df.loc[df[\"ID_DOKUMENT\"]==15997, [\"text\"]]\n",
    "# sent2_df\n",
    "# df.loc[:10, [\"text\", \"VORNAME\", \"ID_DOKUMENT\"]]\n",
    "# sent2_df.loc[10:30, ['Satz3', 'VORNAME', 'NAME', 'VORNAME_MUTTER', 'NAME_MUTTER', 'VORNAME_VATER', 'NAME_VATER']]\n",
    "\n",
    "# sent2_df['ID_DOKUMENT_string']=sent2_df['ID_DOKUMENT'].apply(str)\n",
    "\n",
    "# sent2_df['Satz100char'] = sent2_df['Satz'].str[:100]\n",
    "\n",
    "# sent2_df['ID_DOKUMENT_Satz100char']=sent2_df['ID_DOKUMENT_string']+sent2_df['Satz100char']\n",
    "\n",
    "# sent2_df = sent2_df.drop_duplicates(subset='ID_DOKUMENT_Satz100char', keep=\"first\")\n",
    "# sent2_df['ID_DOKUMENT_Satz100char'][sent2_df.duplicated(subset=['ID_DOKUMENT_Satz100char'])]\n",
    "# # sent2_df[sent2_df.duplicated(subset=['ID_DOKUMENT_Satz100char'])]\n",
    "\n",
    "# drop duplicates\n",
    "sent2_df = sent2_df.drop_duplicates(subset='Satz', keep=\"first\")\n",
    "# clean up the dataframe\n",
    "sent2_df= sent2_df[['Satz','ID_DOKUMENT', 'ID_K',  'Satz3', 'Satz5', 'ID_COMBO_ANREDE', 'VORNAME', 'NAME', 'VORNAME_MUTTER', 'NAME_MUTTER', 'VORNAME_VATER', 'NAME_VATER' ]]\n",
    "# sent2_df\n",
    "# save the text_sections dataframe in a csv file and excel file\n",
    "sent2_df.sort_values(by=['Satz'],ascending=False).to_excel('GewaltSaetze5_20230213.xlsx',index=False)\n",
    "sent2_df.sort_values(by=['Satz'],ascending=False).to_csv('GewaltSaetze5_20230213.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db6f4e1f",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "036b9fb1130ac84a5bede25763fe06ea1f1f97943a6a1a32fbdde563ce2e1256"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
