{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c913c465",
   "metadata": {},
   "source": [
    "# Preprocessing and Import Word Documents (Reports)\n",
    " \n",
    "## Literatur\n",
    "- https://textmining.wp.hs-hannover.de/Preprocessing.html\n",
    "- https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0\n",
    "\n",
    "## Kommentare\n",
    "- doc lesen ist schwierig. Darum verwenden eines Converters http://www.multidoc-converter.com/en/index.html, der in docx convertiert\n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0e5bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --trusted-host pypi.python.org --trusted-host files.pythonhosted.org --trusted-host pypi.org docx2python --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a4abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge pypdf2\n",
    "# conda install nltk \n",
    "# pip install HanTa\n",
    "\n",
    "import re, nltk, os, glob, docx\n",
    "import pandas as pd\n",
    "import wehs_helpers as wh\n",
    "from HanTa import HanoverTagger as ht\n",
    "from docx2python import docx2python\n",
    "import win32com.client\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import functools\n",
    "import operator\n",
    "# import stopwords\n",
    "# nltk.download('stopwords')  \n",
    "os.name  # nt means windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a12f360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list with pats to all files in the folder and subfolders\n",
    "\n",
    "liste_docs2docx2 = pd.read_csv('liste_docs2docx2.csv',delimiter=';',encoding=\"latin-1\")\n",
    "liste_docs2docx2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87906f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import aspose.words as aw\n",
    "\n",
    "def doc2docx(quelle, ziel):\n",
    "    \"\"\"\n",
    "    Converts a DOC file to DOCX format using the Aspose.Words library.\n",
    "\n",
    "    Parameters:\n",
    "    - quelle (str): The path of the input DOC file.\n",
    "    - ziel (str): The path where the converted DOCX file will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - None: If the conversion is successful.\n",
    "    - RuntimeError: If an error occurs during the conversion.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load DOC file\n",
    "        doc = aw.Document(quelle)\n",
    "        # Save DOC as DOCX\n",
    "        doc.save(ziel)\n",
    "    except RuntimeError:\n",
    "        return\n",
    "\n",
    "\n",
    "for i in range(len(liste_docs2docx2)):\n",
    "        # Convert each document from .doc to .docx format\n",
    "        doc2docx(liste_docs2docx2.loc[i, \"quelle_mit_pfad\"], liste_docs2docx2.loc[i, \"ziele_mit_pfad\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9cff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def get_meta(region):\n",
    "    \"\"\"\n",
    "    Retrieves metadata for rechenschaftsberichte based on the given region.\n",
    "\n",
    "    Args:\n",
    "        region (str): The region for which to retrieve the metadata.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The metadata for the rechenschaftsberichte in the specified region.\n",
    "    \"\"\"\n",
    "    def docpath(filename, folder):\n",
    "        \"\"\"\n",
    "        Constructs the document path based on the filename and folder.\n",
    "\n",
    "        Args:\n",
    "            filename (str): The name of the file.\n",
    "            folder (str): The folder path.\n",
    "\n",
    "        Returns:\n",
    "            str: The constructed document path.\n",
    "        \"\"\"\n",
    "        if len(filename.split('.')) != 2:\n",
    "            return 'error'\n",
    "        else:\n",
    "            name, ext = filename.split('.')\n",
    "\n",
    "        if ext == 'DOCM':\n",
    "            path = os.path.join(folder, filename)\n",
    "        elif ext == 'DOC':\n",
    "            path = os.path.join(folder, 'docx', name + '.docx')\n",
    "        else:\n",
    "            return 'error'\n",
    "        \n",
    "        if os.path.exists(path):\n",
    "            return path\n",
    "        else:\n",
    "            return 'error'\n",
    "        \n",
    "    folder = 'c:\\\\temp\\\\rechenschaftsberichte\\\\rbs_{}'.format(region)\n",
    "    meta_path = 'c:/temp/rechenschaftsberichte/meta_infos_rbs_{}.csv'.format(region)\n",
    "    df = pd.read_csv(meta_path, delimiter=';')\n",
    "    df['region'] = region\n",
    "    df['docpath'] = df['RB_DATEI_NAME'].apply(lambda n: docpath(n, folder))\n",
    "    df['doctype'] = df['docpath'].apply(lambda p: p.split('.')[-1])\n",
    "    return df\n",
    "\n",
    "regions = ['norden', 'osten', 'sueden', 'westen']\n",
    "df = pd.concat([get_meta(reg) for reg in regions])\n",
    "errors = df[df.docpath == 'error']['DOKUMENTNAME'].values\n",
    "print('Number of errors =', len(errors))\n",
    "df = df.query(\"docpath!='error'\").reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8338b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the number of documents per region and doctype\n",
    "wh.two_count(df,'doctype','region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8b94c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting text from the documents and save them in chunks of 1000 documents each as csv files\n",
    "import docx\n",
    "import functools\n",
    "import operator\n",
    "\n",
    "def get_text(docpath, docout=False):\n",
    "    \"\"\"\n",
    "    Extracts text from a given document file.\n",
    "\n",
    "    Parameters:\n",
    "    - docpath (str): The path of the document file.\n",
    "    - docout (bool): Optional. If True, returns the document object instead of the extracted text.\n",
    "\n",
    "    Returns:\n",
    "    - str or Document: The extracted text from the document file, or the document object if docout is True.\n",
    "    \"\"\"\n",
    "\n",
    "    if docpath.split('.')[-1].lower() == 'docx':\n",
    "        # If the document file is in .docx format\n",
    "        doc = docx.Document(docpath)\n",
    "        # Extract the text from paragraphs, excluding empty paragraphs\n",
    "        text = '\\n'.join([p.text for p in doc.paragraphs if len(p.text) > 0])\n",
    "    elif docpath.split('.')[-1].lower() == 'docm':\n",
    "        # If the document file is in .docm format\n",
    "        doc = docx2python(docpath)\n",
    "        tmp = doc.body\n",
    "        for ii in range(3):\n",
    "            tmp = functools.reduce(operator.iconcat, tmp, [])\n",
    "        # Join the text from the document body\n",
    "        text = '\\n'.join(tmp)\n",
    "    else:\n",
    "        # If the document file format is not supported\n",
    "        return 'error'\n",
    "\n",
    "    if docout:\n",
    "        # If docout is True, return the document object\n",
    "        return doc\n",
    "    else:\n",
    "        # Remove non-ASCII characters from the extracted text\n",
    "        text = ''.join(char for char in text if ord(char) < 256)\n",
    "        return text\n",
    "    \n",
    "def get_tags(text, tokenizer = nltk.tokenize.RegexpTokenizer(r'[a-zA-ZäöüÄÖÜ]+')):\n",
    "\n",
    "    tokens = [token.lower() for token in tokenizer.tokenize(text)]\n",
    "    tagger = ht.HanoverTagger('morphmodel_ger.pgz')\n",
    "    return tagger.tag_sent(tokens, taglevel=1)\n",
    "\n",
    "\n",
    "# pattern = r'NN' # r'NN|VV.*|ADJA'\n",
    "def get_lemmas(tags, pattern):\n",
    "     return [lemma for (token, lemma, pos) in tags if re.match(pattern,pos)]\n",
    "    \n",
    "Nchunk = 1000   \n",
    "Ndoc = df.shape[0]\n",
    "pathpattern = 'data/chunk_{:02d}.csv'\n",
    "for ii in range(int(Ndoc/Nchunk)+1):\n",
    "    ind1 = ii*Nchunk\n",
    "    ind2 = min([Ndoc,(ii+1)*Nchunk])\n",
    "    sdf = df.iloc[ind1:ind2]\n",
    "    sdf.loc[sdf.index,'text'] = sdf['docpath'].apply(get_text)\n",
    "    \n",
    "    # tags = sdf.text.apply(get_tags)\n",
    "    # sdf.loc[sdf.index,'nn_lemmas'] = tags.apply(lambda t: get_lemmas(t,'NN'))\n",
    "    # sdf.loc[sdf.index,'vv_lemmas'] = tags.apply(lambda t: get_lemmas(t,'VV.*'))\n",
    "    # sdf.loc[sdf.index,'adja_lemmas'] = tags.apply(lambda t: get_lemmas(t,'ADJA'))\n",
    "    \n",
    "    sdf.to_csv(pathpattern.format(ii))\n",
    "    print('chunk',ii,'done ...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "036b9fb1130ac84a5bede25763fe06ea1f1f97943a6a1a32fbdde563ce2e1256"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
